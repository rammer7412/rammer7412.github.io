---
title: "[Linear Algebra] 선형대수학(2강) - 내적과 선형결합"
description: >-
  벡터의 내적을 계산할 수 있습니다. 또한 선형결합을 이해하고 선형독립과 선형종속을 구분할 수 있습니다.
author: rammer
date: 2025-06-30 00:01:00 +0900
permalink: /posts/linear_algebra_2/
categories: [Linear Algebra]
tags: [Linear Algebra,Mathematics]
use_math: true
toc: true
pin: false
media_subpath: '/posts/20250630'

---
  * 수식이 제대로 보이지 않는다면, 새로고침(F5)을 해주시기 바랍니다.  

이번 포스팅에서는 벡터의 내적, 그리고 선형결합을 다루겠습니다. 벡터의 내적의 계산은 고등학교 수학에서도 다루는 내용인데요, 여기서는 뿐만 아니라 내적의 성질 및 관련 증명도 다뤄보겠습니다. 또한 오늘의 가장 중요한 개념, 선형결합도 이해를 하셔야 합니다.<br>

## Inner Product
벡터의 내적(inner product, 또는 dot product)의 연산은 다음처럼 정의됩니다.<br>
$\mathbf{u} = (u_1, u_2, \dots, u_n), \quad
\mathbf{v} = (v_1, v_2, \dots, v_n)  \in \mathbb{R}^n$에 대하여,<br>
$\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_i v_i = \mathbf{u}^T \mathbf{v}$

$$
\mathbf{u} = (u_1, u_2, u_3), \quad
\mathbf{v} = (v_1, v_2, v_3)
$$

$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + u_3 v_3
$$

<br>아래는 내적 연산의 예시입니다.<br><br>
$$
\mathbf{u}^T = 
\begin{bmatrix}
u_1 & u_2 & u_3
\end{bmatrix}, \quad
\mathbf{v} = 
\begin{bmatrix}
v_1 \\
v_2 \\
v_3
\end{bmatrix}
\Rightarrow
\mathbf{u}^T \mathbf{v} = u_1 v_1 + u_2 v_2 + u_3 v_3
$$
<br>

## **Properties of inner product**
다음은 정말 중요한 내적 연산의 성질 5가지입니다. 당연해 보이지만, 정말 중요한 성질이니 꼭 기억해둡시다.<br>

$$
\text{1. Commutativity:} \quad \mathbf{u} \cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{u}
$$

$$
\text{2. Scalar associativity:} \quad (c\mathbf{u}) \cdot \mathbf{v} = c (\mathbf{u} \cdot \mathbf{v}) \quad \text{for all } c \in \mathbb{R}
$$

$$
\text{3. Distributivity:} \quad \mathbf{u} \cdot (\mathbf{v} + \mathbf{w}) = \mathbf{u} \cdot \mathbf{v} + \mathbf{u} \cdot \mathbf{w}
$$

$$
\text{4. Positive semi-definiteness:} \quad \mathbf{u} \cdot \mathbf{u} = \mathbf{u}^T \mathbf{u} \geq 0
$$

$$
\text{5. Zero inner product condition:} \quad \mathbf{u}^T \mathbf{u} = 0 \iff \mathbf{u} = \mathbf{0}
$$

## **Norm**
이번에는 Norm을 다루겠습니다. **노름(norm)**은 벡터의 길이 또는 크기(magnitude)를 측정하는 함수입니다. 가장 대표적인 예는 유클리드 노름(Euclidean norm) 또는 2-norm으로, 우리가 일반적으로 인식하는 거리 개념과 같습니다.<br><br>

$$
\|\mathbf{u}\| = \sqrt{u_1^2 + u_2^2 + \cdots + u_n^2} = \sqrt{\mathbf{u} \cdot \mathbf{u}} = \sqrt{\mathbf{u}^T \mathbf{u}}
$$

<br>또한 아래의 norm이 되기 위한 조건 3가지를 꼭 기억해두시길 바랍니다. Norm의 3가지 공리라고도 불립니다.<br><br>

$$
\text{1. Positive definiteness:} \quad \|\mathbf{u}\| \geq 0 \quad \text{and} \quad \|\mathbf{u}\| = 0 \iff \mathbf{u} = \mathbf{0}
$$

$$
\text{2. Homogeneity (absolute scalability):} \quad \|c \mathbf{u}\| = |c| \cdot \|\mathbf{u}\| \quad \text{for all } c \in \mathbb{R}
$$

$$
\text{3. Triangle inequality:} \quad \|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|
$$

## **Other Norms**
위에서 다룬 norm은 2-norm입니다. 그 외에도 다른 종류의 norm이 있는데요, 간단하게 소개해드리겠습니다.<br>
### 1-norm
1-norm은 각 성분의 절댓값을 모두 더한 값으로 정의됩니다. 맨해튼 거리(Manhattan distance)로 불리기도 합니다.<br>
<div align="center">
$$
\displaystyle
\|\mathbf{u}\|_1 = \sum_{i=1}^{n} |u_i|
$$
</div>

### p-norm
p-노름은 각 성분의 절댓값을 p제곱한 후 모두 더한 뒤, 그 전체에 대해 p제곱근을 취한 값입니다. 여기서 중요한 점은 p≥1일 때에만 norm으로 인정됩니다.<br>
<div align="center">
$$
\displaystyle
\|\mathbf{u}\|_p = \left( \sum_{i=1}^{n} |u_i|^p \right)^{1/p}, \quad p \geq 1
$$
</div>

### infinity norm
infinity norm은 벡터의 성분 중 절댓값이 가장 큰 값을 의미합니다. 아래와 같이 표기합니다.<br>
<div align="center">
$$
\displaystyle
\|\mathbf{u}\|_\infty = \max_{1 \leq i \leq n} |u_i|
$$
</div>
<br>
지금까지 vector의 norm에 대해 살펴보았습니다. matrix에 대한 norm(matrix norm, Frobenius norm)도 존재하지만, 여기서는 다루지 않겠습니다. 궁금하신 분들은 아래 링크를 참고하세요.
- matrix norm에 대한 자세한 설명은 [여기](https://en.wikipedia.org/wiki/Matrix_norm)를 참고하세요.
- Frobenius norm에 대한 자세한 설명은 [여기](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)를 참고하세요.

## **Linear Combination**
오늘 다룰 내용 중에 간단해 보이면서도 매우 매우 중요한 개념, 바로 선형 결합(linear combination)입니다. 앞으로도 계속 등장할 개념이자, 핵심 개념 중 하나이기 때문에 반드시 알고 넘어가셔야 합니다.<br>
벡터 공간에서 벡터 $ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k $와 스칼라 $ c_1, c_2, \dots, c_k \in \mathbb{R} $가 주어졌을 때,<br>  
다음과 같은 형태로 벡터를 결합한 것을 선형 결합(linear combination)이라고 합니다:<br>
$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k
$$
<br>

### Example
두 벡터 $ \mathbf{v}_1 = (1, 0),\ \mathbf{v}_2 = (0, 1) $를 생각해봅시다.<br>
이들은 각각 $x$축 방향 단위 벡터, $y$축 방향 단위 벡터입니다.<br>
즉, 각각 오른쪽으로 1칸 이동, 위쪽으로 1칸 이동하는 방향을 나타냅니다.

이제 어떤 벡터 $ \mathbf{v} = (x, y) \in \mathbb{R}^2 $를 표현해 보겠습니다.

<br>

$ \mathbf{v} $는 $x$축 방향으로 $x$만큼, $y$축 방향으로 $y$만큼 이동한 점이므로,  
다음과 같은 선형 결합(linear combination)으로 표현됩니다:

$$
\mathbf{v} = x \cdot \mathbf{v}_1 + y \cdot \mathbf{v}_2
$$

<br>

예를 들어, $ \mathbf{v} = (3, 2) $일 경우에는 다음처럼 계산됩니다:

$$
\mathbf{v} = 3 \cdot (1, 0) + 2 \cdot (0, 1) = (3, 0) + (0, 2) = (3, 2)
$$
<br>

## **Linear Independence**
linear combination을 이해하셨다면, 이번에 이해하실 내용은 선형독립입니다.<br>
벡터 집합 $ \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k \} $가 주어졌을 때, 이 벡터들이 선형 독립(linearly independent)이라는 것은 다음 조건을 만족함을 의미합니다:<br>
$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k = \mathbf{0}
\quad \Rightarrow \quad
c_1 = c_2 = \cdots = c_k = 0
$$

즉, 이 벡터들의 선형 결합이 0벡터가 되는 유일한 방법은  
**모든 계수 $c_i$가 0이 되는 경우 뿐**이라는 뜻입니다.
<br>
만약 위 조건을 만족하지 않고, 모든 계수가 0이 아닌 조합으로도 0벡터가 만들어질 수 있다면, 그 벡터 집합은 **선형 종속(linearly dependent)**이라고 합니다.

즉, 어떤 벡터 하나가 나머지 벡터들의 선형 결합으로 표현될 수 있다면, 그 집합은 선형 종속입니다.

### Example
벡터 $ \mathbf{v}_1 = (1, 2),\ \mathbf{v}_2 = (2, 4) $를 생각해보면,

$$
2 \cdot \mathbf{v}_1 - 1 \cdot \mathbf{v}_2 = (2, 4) - (2, 4) = (0, 0)
$$

이 경우 $c_1 = 2$, $c_2 = -1$로 0벡터가 만들어졌으므로  
$ \mathbf{v}_1, \mathbf{v}_2 $는 선형 종속입니다.

## **Conclusion**
본문에서는 설명드리지 않은 내용이 하나 더 있습니다. linearly independent한 벡터들을 선형결합하여 어떤 벡터를 만든다고 해봅시다. 각 벡터 앞 마다 계수가 존재할텐데요, 그러한 계수를 결정하는 방법은 오직 하나(unique)입니다. 아주 중요한 내용이니 꼭 기억해두시길 바랍니다. 이 내용의 증명은 연습 문제를 통해 직접 해보시길 바랍니다.<br>



## **Practice** 
Q1. 벡터 $\mathbf{u} = (3, -4, 1)$에 대하여, 그 유클리드 노름(Euclidean norm)을 계산하시오.<br>
Q2. 벡터 공간 $\mathbb{R}^n$에서 벡터들 $\mathbf{v}_1, \dots, \mathbf{v}_k \in \mathbb{R}^n$이 선형 독립이라고 하자. $\mathbf{x} \in \mathbb{R}^n$가 다음과 같은 형태로 주어진다:

$$
\mathbf{x} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k
$$

이때, $\mathbf{x}$를 표현하는 선형결합의 계수들이 유일(unique)하게 결정됨을 증명하시오.<br>

### Answer
추후에 업로드 예정
