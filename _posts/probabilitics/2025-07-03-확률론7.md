---
title: "[Probabilistics] 확률 이론(7강) - 대표적인 확률 분포 - 2 (continuous)"
description: >-
  대표적인 연속 확률 분포의 종류와 식을 이해하고, 이를 활용할 수 있습니다.
author: rammer
date: 2025-07-03 00:10:00 +0900
permalink: /posts/prob_7/
categories: [확률 이론]
tags: [Probability,Mathematics]
use_math: true
toc: true
pin: false
media_subpath: '/posts/20250703'

---
  * 수식이 제대로 보이지 않는다면, 새로고침(F5)을 해주시기 바랍니다.  
  
  
 대표적인 확률 분포들을 common families of distributions이라고 합니다. 지난 강의(6강)에서는 discrete distributions를 다루었습니다. 이번 시간에는 continuous distributions를 다루겠습니다.


## **Continuous Uniform distribution**
$X \sim \mathrm{Unif}(a, b), \quad a < b, \quad a, b \in \mathbb{R}$<br><br>
Continuous Uniform 분포는 구간 $(a, b)$ 내의 모든 실수값이 동일한 확률 밀도를 갖는 연속 확률 분포입니다.
즉, $X$는 $a$와 $b$ 사이의 모든 실수 값에서 균등하게 선택될 때 다음 분포를 따릅니다.<br><br>
$f_X(x) = \dfrac{1}{b - a}, \quad x \in [a, b]$<br>

### Expectation & Variance
$\mathbb{E}(X) = \dfrac{a + b}{2}$<br>
$Var(X) = \dfrac{(b - a)^2}{12}$<br>
  
  
  참고로 continuous uniform 분포는 무작위 샘플링이나, 난수 생성기 등에서도 주로 사용됩니다.

## **Gaussian(Normal) distribution**
$X \sim \mathcal{N}(\mu, \sigma^2), \quad \mu \in \mathbb{R}, \sigma > 0$<br><br>
오늘 다룰 연속 확률 분포들은 모두 중요하지만, 그 중에서도 가장 중요한 가우시안 분포입니다. 정말 가우시안 분포는 등장하지 않는 곳이 없을만큼 중요한 분포이므로, 반드시 이해하고 넘어가셔야 합니다. pdf 식도 외워두시는 것을 추천합니다.<br>
아직은 확률 변수를 여러 개 가진 pdf를 다루지 않았습니다. 이후에 확률 변수 N개가 존재하는 N-dim gaussian도 다룰 예정이니 참고바랍니다.<br>
<br>
Gaussian 분포(정규분포)는 평균 $\mu$를 중심으로 대칭적인 종 모양의 연속 확률 분포입니다. [여기](https://en.wikipedia.org/wiki/Normal_distribution)에서 가우시안 함수의 모양을 확인하세요.<br>
가우시안 분포는 자연 현상, 측정값, 오차 모델링 등에서 가장 널리 사용되는 분포입니다. 함수 식은 다음과 같습니다.<br><br>

$$
f_X(x) = \dfrac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\dfrac{(x - \mu)^2}{2\sigma^2} \right), \quad x \in \mathbb{R}
$$
<br><br>

### Expectation & Variance
$\mathbb{E}(X) = \mu$<br>
$Var(X) = \sigma^2$<br>

<br>※ $\mu = 0, \sigma^2 = 1$일 경우 표준 정규분포(standard normal)라고 하며,
이때는 $X∼N(0,1)$, 간단히 $Z$라고 표기합니다.

## **Standard Normal Distribution: 면적이 1임을 증명**
가우시안 분포는 수학적으로도 매우 중요하기 때문에 조금 더 깊게 다뤄보겠습니다. 가우시안 함수의 아래 면적이 1이 됨을 먼저 증명해보겠습니다. <br>
proof:  
표준 정규분포는 다음과 같은 확률밀도함수를 가집니다.

$$
f(x) = \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2}
$$

<br>확률밀도함수는 전체 구간에서 면적이 1이어야 하므로, 아래와 같은 적분이 성립해야 합니다:

$$
\int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2} \, dx = 1
$$

<br>이를 증명하기 위해 먼저 다음 가우시안 적분을 계산합니다:

$$
A = \int_{-\infty}^{\infty} e^{-x^2} dx
$$

<br>위 적분은 직접 계산이 어려우므로, 제곱한 후 이중적분으로 바꿔 계산합니다:

$$
A^2 = \left( \int_{-\infty}^{\infty} e^{-x^2} dx \right) \left( \int_{-\infty}^{\infty} e^{-y^2} dy \right)
= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2 + y^2)} dx\,dy
$$

<br>이중적분을 극좌표계로 바꾸면 다음과 같습니다:

$ x^2 + y^2 = r^2 $

$$
dx\,dy = |J| = r, \quad dr\,d\theta = r\,dr\,d\theta
$$

<br>여기서 Jacobian이 무엇인지 모르실 분도 계실 것이라 생각하여 설명을 드리겠습니다.<br>
이차원 변수 치환에서는 면적 요소도 함께 변환되어야 하며, 이때 사용되는 것이 바로 야코비안(Jacobian)입니다.<br>
예를 들어, 다음과 같이 변수 치환을 한다고 합시다:

$$
x = x(r, \theta), \quad y = y(r, \theta)
$$

이 경우, 변수 $(x, y)$ 공간의 면적 요소 $dx\,dy$는  
새로운 변수 $(r, \theta)$ 공간에서는 단순히 $dr\,d\theta$로 바꿀 수 없습니다. 왜냐하면, 단위 길이 변화가 두 공간에서 면적 단위로 다르게 퍼지기 때문입니다. 즉, 좌표계가 바뀌면 면적도 늘어나거나 줄어들 수 있으므로, 그 크기 변화를 보정해줄 scale 계수가 필요합니다.<br>  
그게 바로 Jacobian 행렬의 절댓값입니다.
변수 치환 $(x, y) \to (r, \theta) $일 때:

$$
x = r \cos\theta, \quad y = r \sin\theta
$$

<br>야코비안 행렬은 다음과 같이 정의됩니다:

$$
J = 
\begin{bmatrix}
\dfrac{\partial x}{\partial r} & \dfrac{\partial x}{\partial \theta} \\\\
\dfrac{\partial y}{\partial r} & \dfrac{\partial y}{\partial \theta}
\end{bmatrix}
=
\begin{bmatrix}
\cos\theta & -r \sin\theta \\\\
\sin\theta & r \cos\theta
\end{bmatrix}
$$

<br>determinant가 무엇인지 모르신다면 저의 선형대수학 강의를 참고하시길 바랍니다. J의 determinant의 절댓값을 구하면 다음과 같습니다:

$$
\left| \det J \right| = \left| r \cos^2\theta + r \sin^2\theta \right| = r
$$

<br>따라서 면적 요소는 다음과 같이 변환됩니다:

$$
dx\,dy = |J| \, dr\,d\theta = r\,dr\,d\theta
$$

<br>치환의 결과를 모두 구했으니, 초반에 계산했던 식에 이를 적용해보겠습니다.<br>

$$
A^2 = \int_0^{2\pi} \int_0^{\infty} e^{-r^2} \cdot r \, dr \, d\theta
$$

<br>먼저 내적분부터 계산한 결과는 다음과 같습니다.

$$
\int_0^{\infty} r e^{-r^2} dr = \left[ -\dfrac{1}{2} e^{-r^2} \right]_0^{\infty} = \dfrac{1}{2}
$$

<br>그 다음은 외적분의 계산 결과입니다.

$$
\int_0^{2\pi} d\theta = 2\pi
$$

<br>따라서 전체 적분은:

$$
A^2 = 2\pi \cdot \dfrac{1}{2} = \pi \quad \Rightarrow \quad A = \sqrt{\pi}
$$

<br>우리가 원한 적분은 다음과 같습니다:

$$
\int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
$$

<br>변수 치환:

$$
x = \sqrt{2} u \quad \Rightarrow \quad dx = \sqrt{2} du
$$

<br>이를 이용하면:

$$
\int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
= \int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-u^2} \cdot \sqrt{2} du
= \dfrac{\sqrt{2}}{\sqrt{2\pi}} \cdot \int_{-\infty}^{\infty} e^{-u^2} du
= \dfrac{\sqrt{2}}{\sqrt{2\pi}} \cdot A
= \dfrac{\sqrt{2}}{\sqrt{2\pi}} \cdot \sqrt{\pi}
= 1
$$

$$
\int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = 1
$$

<br>따라서, 표준 정규분포의 확률밀도함수는 전체 구간에서 면적이 1이며, pdf로서의 성질을 만족함을 증명할 수 있습니다.<br>

혹시 1강에서 다룬 gamma function을 기억하시나요? 그 중 3번 째 성질이 바로 $\Gamma(\frac{1}{2})=\sqrt{\pi}$였습니다. gaussian distribution의 pdf의 전체 구간에서의 면적이 1임을 활용하여 이 사실을 어렵지 않게 증명할 수 있습니다. 연습 문제를 통해 직접 증명해보시길 바랍니다.<br>

## **Exponential distribution**  
$$
f_X(x) =
\begin{cases}
\displaystyle \frac{1}{\beta} e^{-x / \beta} & \text{if } x \ge 0 \\
0 & \text{otherwise}
\end{cases}
$$

Exponential 분포는 어떤 사건이 일어날 때까지 걸리는 시간 또는 두 사건 사이의 시간 간격을 모델링하는 연속 확률 분포입니다. Poisson process에서 연속적인 시간 간격을 설명하는 데 자주 사용됩니다. exponential distribution의 pdf 식은 다음과 같습니다.<br>  

$$
f_X(x) =
\begin{cases}
\dfrac{1}{\beta} e^{-x / \beta} & \text{if } x \geq 0 \\\\
0 & \text{if } x < 0
\end{cases}
$$

### Expectation & Variance  
$
\mathbb{E}(X) = \beta
$

$
Var(X) = \beta^2
$

<br>
이번에는 $\mathbb{E}(X) = \beta$임을 직접 증명해보겠습니다.<br>
proof:  
감마 함수 꼴로 바꾸기 위해 다음과 같이 치환합니다.

- $u = \frac{x}{\beta} \Rightarrow x = \beta u,\; dx = \beta du$

<br>대입하면 다음과 같이 됩니다.

$$
\mathbb{E}(X)
= \int_0^{\infty} \beta u \cdot \dfrac{1}{\beta} e^{-u} \cdot \beta \, du
= \beta \int_0^{\infty} u e^{-u} \, du
= \beta \cdot \Gamma(2) = \beta \cdot 1! = \beta
$$
<br>

## **Gamma distribution**  
$X \sim \mathrm{Gamma}(\alpha, \beta), \quad \alpha > 0,\; \beta > 0$<br><br>  
Gamma 분포는 양의 연속적인 사건의 누적 합(대기 시간)을 모델링하는 분포로, Exponential 분포의 일반화된 형태입니다.  
즉, Exponential 분포가 한 번의 사건 발생까지의 시간이라면, Gamma 분포는 $\alpha$번의 사건이 누적되기까지의 총 시간을 의미합니다.<br>  
pdf는 다음과 같습니다:  

$$
f_X(x) =
\begin{cases}
\dfrac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x / \beta} & \text{if } x > 0 \\\\
0 & \text{if } x \leq 0
\end{cases}
$$

---

### Expectation & Variance

$\mathbb{E}(X) = \alpha \beta$

$Var(X) = \alpha \beta^2$

이번에는 $\mathbb{E}(X) = \alpha \beta$임을 증명해보겠습니다.  
proof:  

기대값 $\mathbb{E}(X)$은 정의에 따라 다음과 같이 적분으로 표현됩니다.

$$
\mathbb{E}(X) = \int_0^\infty x \cdot f_X(x) \, dx
= \int_0^\infty x \cdot \dfrac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x / \beta} dx
= \dfrac{1}{\Gamma(\alpha) \beta^\alpha} \int_0^\infty x^\alpha e^{-x / \beta} dx
$$

<br>감마 함수 꼴로 바꾸기 위해 다음과 같이 치환합니다:

- $u = \dfrac{x}{\beta} \Rightarrow x = \beta u,\; dx = \beta du$

<br>치환 후 적분은 다음과 같이 바뀝니다:

$$
\mathbb{E}(X)
= \dfrac{1}{\Gamma(\alpha) \beta^\alpha} \int_0^\infty (\beta u)^\alpha e^{-u} \cdot \beta du
= \dfrac{\beta^{\alpha+1}}{\Gamma(\alpha) \beta^\alpha} \int_0^\infty u^\alpha e^{-u} du
= \dfrac{\beta}{\Gamma(\alpha)} \cdot \Gamma(\alpha + 1)
$$

여기서 gamma function의 성질인 $\Gamma(\alpha + 1) = \alpha \cdot \Gamma(\alpha)$을 사용해주면,  

$$
\mathbb{E}(X) = \beta \cdot \alpha = \alpha \beta
$$

따라서 $\mathbb{E}(X) = \alpha \beta$임이 증명되었습니다.

## **Beta distribution**  
$X \sim \mathrm{Beta}(\alpha, \beta), \quad \alpha > 0,\; \beta > 0$<br><br>  
Beta 분포는 확률 변수 $ X \in (0, 1)$ 구간에서 정의되는 연속 확률 분포로, 사건의 비율, 확률, 비중 등을 모델링할 때 자주 사용됩니다. <br>  
pdf는 다음과 같습니다:

$$
f_X(x) =
\begin{cases}
\dfrac{1}{B(\alpha, \beta)} x^{\alpha - 1} (1 - x)^{\beta - 1} & \text{if } 0 < x < 1 \\\\
0 & \text{otherwise}
\end{cases}
$$

<br>여기서 $B(\alpha, \beta)$는 **베타 함수(Beta function)**로, 감마 함수로 다음과 같이 정의됩니다:

$$
B(\alpha, \beta) = \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1} dt = \dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
$$

이번에는 베타 함수가 위와 같이 감마 함수로 표현될 수 있음을 증명해보겠습니다. 꽤 난해하니 잘 따라오시길 바랍니다.<br>
proof:  

감마 함수는 다음과 같이 정의됩니다.

$$
\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1} e^{-x} dx, \quad
\Gamma(\beta) = \int_0^\infty y^{\beta - 1} e^{-y} dy
$$

<br>따라서 두 감마 함수의 곱은 다음과 같은 이중 적분이 됩니다.

$$
\Gamma(\alpha)\Gamma(\beta) = \int_0^\infty \int_0^\infty x^{\alpha - 1} y^{\beta - 1} e^{-(x + y)} dx\,dy
$$

<br>그 다음은 아래처럼 같이 변수 치환을 합니다.

- $x = s t,\quad y = s(1 - t)$
- $s \in (0, \infty),\quad t \in (0, 1)$

<br>그 다음은 야코비안(Jacobian)을 계산해줍니다. 계산 결과는 아래와 같습니다.

$$
\left| \dfrac{\partial(x, y)}{\partial(s, t)} \right| = s
$$

<br>그 다음으로 치환 결과를 대입하면 아래처럼 됩니다.

$$
\Gamma(\alpha)\Gamma(\beta)
= \int_0^1 \int_0^\infty (s t)^{\alpha - 1} (s(1 - t))^{\beta - 1} e^{-s} \cdot s \, ds\,dt
$$

<br>이를 정리하면:

$$
= \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1} \left[ \int_0^\infty s^{\alpha + \beta - 1} e^{-s} ds \right] dt
= \Gamma(\alpha + \beta) \cdot \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1} dt
$$

<br>따라서 아래와 같이 베타 함수는 감마 함수들로 표현이 가능합니다.

$$
\Gamma(\alpha)\Gamma(\beta) = \Gamma(\alpha + \beta) \cdot B(\alpha, \beta)
\quad \Rightarrow \quad
B(\alpha, \beta) = \dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
$$

### Expectation & Variance

$\mathbb{E}(X) = \dfrac{\alpha}{\alpha + \beta}$

$Var(X) = \dfrac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

## **Conclusion**
이번 포스팅에서는 common families of distributions 중, continuous한 확률 분포들에 대해 다루었습니다. 증명도 복잡하고, 식도 어려워서 이번 강의는 특히 어렵게 느껴지셨을 수도 있다고 생각합니다. 그리고 역대급으로 분량이 가장 많았던 것 같기도 하네요...<br>
글을 여러 번 반복하여 읽으시면서, 수식이 익숙해지도록 증명을 손으로 따라 써가면서 공부하시는 것을 추천드립니다. 또한 위 확률 분포를 모두 이해하였고, 이를 활용한 계산이나 증명 등을 할 수 있는지를 아래 연습 문제를 통해 확인해보시길 바랍니다. 이번 글은 여기서 마치겠습니다.<br>

## **Practice** 
Q1. $X \sim Unif[a,b]$일 때, $Var(X)$를 구하시오.<br>
Q2. $\Gamma(\frac{1}{2})=\sqrt{\pi}$임을 증명하시오.<br>
Q3. $X \sim \mathcal{N}(0, 1) $일 때, 양의 정수 $k \in \mathbb{N}$에 대해 $\mathbb{E}[X^{2k+1}]$을 구하시오.<br>
Q4. $X \sim \mathrm{Beta}(\alpha, \beta)$일 때, 양의 정수 $r \in \mathbb{N}$에 대하여 $\mathbb{E}[X^r]$을 구하시오.<br>

### Answer 
추후 업로드 예정







