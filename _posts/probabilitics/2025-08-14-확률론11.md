---
title: "[Probabilistics] 확률 이론(11강) - Multivariate Transformation"
description: >-
  다변량 변환의 방법을 이해하고, discrete/continuous한 확률 변수에 대해 다변량 변환을 직접 할 수 있습니다.
author: rammer
date: 2025-08-14 00:55:12 +0900
permalink: /posts/prob_11/
categories: [확률 이론]
tags: [Probability,Mathematics]
use_math: true
toc: true
pin: false
media_subpath: '/posts/20250814'

---
  * 수식이 제대로 보이지 않는다면, 새로고침(F5)을 해주시기 바랍니다.  
  
  
 이번 시간에는 Multivariate Transformation, 그 중에서도 Bivariate Transformation을 중심으로 다변량 변환을 알아보겠습니다.

---

## **Intro: Multivariate Transformation**
앞서 확률이론 9강에서 Univariate Transformation을 다뤘습니다. 변수 하나를 변환하는 방법을 알아봤었는데요, 이번에는 변수가 2개 이상일 때는 어떻게 변환을 하는 지 알아보겠습니다. Multivariate Transformation. 다변량 변환(또는 다변수 변환)이라고 합니다. 이번 포스팅에서는 변수가 2개인 경우(Bivariate Transformation)을 중심으로 다루겠습니다.<br>
<br>
다변량 변환은 확률 변수가 discrete한 지, continuous한 지에 따라 방법이 조금 차이가 있으므로, 명확히 구분하여 기억하시길 바랍니다!<br>

## **Bivariate Transformation: Discrete**
먼저 이산적인 경우를 다루겠습니다. 단계는 다음과 같습니다.  
STEP 1: $f_{X,Y}(x,y)$를 구한다.  
STEP 2: $X=h_1(U,V)$, $Y=h_2(U,V)$를 구한다. (역함수로 볼 수 있습니다.)  
STEP 3: $f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),h_2(u,v))$를 구한다.  
STEP 4 $f_U(u)$, $f_V(v)$를 구한다.  
<br>
  
<br>
변환을 하는 STEP만 보시면 이해가 바로 안가실 수 있을 것 같아서 이해에 도움이 되는 예시를 가져왔습니다. 예시를 따라가면서 변환의 과정을 살펴보시길 바랍니다.<br>

### Example
$X \sim \mathrm{Poisson}(\lambda)$, $Y \sim \mathrm{Poisson}(\theta)$이고 서로 독립이라고 하자.  
$U = X + Y,\; V = Y$일 때, **marginal pmf** $f_U(u)$를 구하시오.  
<br>
풀이:<br>
정의로부터 $V=Y$이므로
$$
f_V(v)=P(V=v)=P(Y=v)=e^{-\theta}\,\frac{\theta^{\,v}}{v!},\qquad v=0,1,2,\dots
$$
<br><br>
독립성$(X \perp\!\!\!\perp Y \;\Longleftrightarrow\; f_{X,Y}(x,y) = f_X(x)\,f_Y(y))$을 이용하여
$$
f_U(u)=P(U=u)=\sum_{x=0}^{u} P\big(X=x,\;Y=u-x\big)
=\sum_{x=0}^{u} P(X=x)\,P(Y=u-x).
$$
<br>
따라서
<br>
$$
\begin{aligned}
f_U(u)
&=\sum_{x=0}^{u} \left(e^{-\lambda}\frac{\lambda^{x}}{x!}\right)
\left(e^{-\theta}\frac{\theta^{\,u-x}}{(u-x)!}\right) \\
&= e^{-(\lambda+\theta)}
\sum_{x=0}^{u} \frac{\lambda^{x}\,\theta^{\,u-x}}{x!\,(u-x)!}.
\end{aligned}
$$
<br>
이때 binomial theorem을 쓰기 위해 $u!$를 곱해 나눠 주면<br>
<br>
$$
\sum_{x=0}^{u} \frac{\lambda^{x}\,\theta^{\,u-x}}{x!\,(u-x)!}
=\frac{1}{u!}\sum_{x=0}^{u} \binom{u}{x}\lambda^{x}\theta^{\,u-x}
=\frac{(\lambda+\theta)^{u}}{u!}.
$$
<br>
따라서
<br>
$$
f_U(u)=e^{-(\lambda+\theta)}\frac{(\lambda+\theta)^{u}}{u!},
\qquad u=0,1,2,\dots
$$

## **Bivariate Transformation: Continuous**
이번에는 연속 확률 변수인 경우를 다루겠습니다. 크게 다르진 않지만 야코비안을 계산하여 곱하는 단계가 하나 더 추가됩니다.<br>
STEP 1:  $f_{X,Y}(x,y)$를 구한다.  
STEP 2: $X=h_1(U,V)$, $Y=h_2(U,V)$를 구한다.  
STEP3: $f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),h_2(u,v))\cdot |J|$를 구한다.  
STEP 4: $f_U(u)$, $f_V(v)$를 구한다.  
  

야코비안에 대해서는 이미 확률 이론 7강에서 깊게 다룬 바가 있습니다. 야코비안이 기억나지 않으시거나 모르시는 분들은 확률 이론 7강을 참고해주시길 바랍니다.<br>
마찬가지로 이번에도 예시를 따라가시면서 연속적인 경우의 Bivariate Transformation을 이해해보시면 좋을 것 같습니다.  

### Example
$X \sim \mathcal{N}(0,1)$, $Y \sim \mathcal{N}(0,1)$ 이고 서로 독립이라 하자.  
$U = X + Y, \quad V = X - Y$일 때, **marginal pdf** $f_U(u)$, $f_V(v)$를 구하시오.  
  
풀이:  
먼저 joint pdf를 구하면 다음과 같습니다. 역시 독립성을 이용하여 쉽게 구할 수 있습니다.<br>
$$
f_{X,Y}(x,y)=f_X(x)f_Y(y)=\frac{1}{2\pi}\exp\!\left(-\frac{x^2+y^2}{2}\right)
$$

다음으로 변수를 주어진 조건에 따라 변환합니다.<br>
$$
U=X+Y,\; V=X-Y 
\quad\Longleftrightarrow\quad
X=\frac{U+V}{2},\; Y=\frac{U-V}{2}
$$
<br>

다음으로 야코비안의 절댓값을 계산해줍니다.<br>
$$
\left|\det\frac{\partial(x,y)}{\partial(u,v)}\right|
=
\left|
\begin{matrix}
\dfrac12 & \dfrac12\\[4pt]
\dfrac12 & -\dfrac12
\end{matrix}
\right|
=\frac12
$$
<br>

그리고 다음과 같은 식을 활용해 계산을 더욱 간단하게 할 수 있습니다.(고등학교 인수분해 단원에서 다룬 내용일 것이라 생각합니다.)<br>
$$
x^2+y^2
=\left(\frac{u+v}{2}\right)^2+\left(\frac{u-v}{2}\right)^2
=\frac{u^2+v^2}{2}.
$$

$$
f_{U,V}(u,v)
= f_{X,Y}\!\left(\tfrac{u+v}{2},\,\tfrac{u-v}{2}\right)\;
\left|\det\frac{\partial(x,y)}{\partial(u,v)}\right|
= \frac{1}{2\pi}\exp\!\left(-\frac{(u^2+v^2)/2}{2}\right)\cdot \frac12
= \frac{1}{4\pi}\exp\!\left(-\frac{u^2+v^2}{4}\right)
$$

확률 변수 $U$와 $V$에 대한 joint pdf를 구했으니, 하나의 변수에 대한 적분을 하면 marginal pdf를 구할 수 있습니다.(확률 이론 10강 내용)<br>
계산 결과는 다음과 같습니다.

$$
f_U(u)=\frac{1}{\sqrt{4\pi}}\,\exp\!\left(-\frac{u^2}{4}\right),
\qquad
f_V(v)=\frac{1}{\sqrt{4\pi}}\,\exp\!\left(-\frac{v^2}{4}\right),
$$

변환의 결과를 살펴보니 $U$와 $V$ 모두 정규 분포를 따름을 알 수 있습니다. $(U \sim \mathcal{N}(0,2),\qquad V \sim \mathcal{N}(0,2))$<br>
실제로 $X~N(0,\sigma^{2}_x)$, $Y~N(0,\sigma^{2}_y)$일 때, $(X+Y)~N(0,\sigma^{2}_x+\sigma^{2}_y)$라는 사실은 매우 유명하며 자주 등장하기 때문에 기억해두시는 것도 나쁠 건 없습니다.<br>

<br>또한 위 결과에서 굉장히 흥미로운 사실이 있는데요, 변환된 joint pdf는 두 marginal pdf의 곱으로 나타낼 수 있음을 확인할 수 있습니다.<br>
$$
f_{U,V}(u,v)
=\underbrace{\frac{1}{\sqrt{4\pi}}e^{-u^2/4}}_{f_U(u)}
\;\underbrace{\frac{1}{\sqrt{4\pi}}e^{-v^2/4}}_{f_V(v)},
$$

이런 경우 **factorizable**하다고 합니다. factorizable하다는 의미는 joint pmf/pdf가 marginal pmf/pdf의 곱 형태로 쓸 수 있음을 의미합니다.
<br> 
$$
X \perp\!\!\!\perp Y \;\Longleftrightarrow\; p_{X,Y}(x,y)=p_X(x)\,p_Y(y)\quad\text{(모든 }(x,y)\text{에서)}
$$
<br>

## **Conclusion**
이번 시간에는 다변량 변환에 대해서 예시와 함께 알아보았습니다. 어렵지 않으나 이 부분은 연습이 필요한 부분이라 생각합니다. discrete인 경우와 continuous인 경우에 다변량 변환의 과정에 차이가 있는데, 연습이 부족하면 충분히 실수하여 놓칠 수 있는 부분도 존재합니다. 아래 연습 문제를 활용하여 직접 다변량 변환을 해보며 방법을 체득하시길 권장합니다. 이번 강의는 여기서 마치겠습니다.

## **Practice** 
Q1. $X \sim \mathrm{Gamma}(\alpha_1,\ \beta)$, $Y \sim \mathrm{Gamma}(\alpha_2,\ \beta)$, $X \perp\!\!\!\perp Y$이고, $U = X + Y,\qquad V = \frac{X}{X+Y}$일 때, $(U,V)$의 joint distribution과 marginal distribution을 각각 구하시오.<br>
Q2. $X \sim \mathrm{Poisson}(\lambda)$, $Y \sim \mathrm{Poisson}(\theta)$, $X \perp\!\!\!\perp Y$일 때, $(X+Y,Y)$의 joint pmf를 구하시오.

### Answer 
추후 업로드 예정







